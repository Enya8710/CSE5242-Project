{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ncu8GfzaYmE",
        "outputId": "bdca67b7-f7e8-41ef-dc4d-0bc32b4e81d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gHOfyOMacgp",
        "outputId": "5b5e838f-6e37-49fb-f457-0cf3bba80e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 47 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 44.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845514 sha256=4576e053f6d3c9daffaf275aa23b0e0aec8d4cce40e5d1734acfed30f67b76e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.6 MB 6.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autofaiss\n",
            "  Downloading autofaiss-2.15.3-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<8,>=6.0.1 in /usr/local/lib/python3.7/dist-packages (from autofaiss) (6.0.1)\n",
            "Requirement already satisfied: fsspec>=2022.1.0 in /usr/local/lib/python3.7/dist-packages (from autofaiss) (2022.10.0)\n",
            "Requirement already satisfied: faiss-cpu<2,>=1.7.2 in /usr/local/lib/python3.7/dist-packages (from autofaiss) (1.7.2)\n",
            "Collecting fire<0.5.0,>=0.4.0\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting dataclasses<1.0.0,>=0.6\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from autofaiss) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5,>=4.62.3 in /usr/local/lib/python3.7/dist-packages (from autofaiss) (4.64.1)\n",
            "Collecting embedding-reader<2,>=1.2.0\n",
            "  Downloading embedding_reader-1.5.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pandas<2,>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from autofaiss) (1.3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire<0.5.0,>=0.4.0->autofaiss) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire<0.5.0,>=0.4.0->autofaiss) (2.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.1.5->autofaiss) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>=1.1.5->autofaiss) (2.8.2)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115940 sha256=6358680b2a851a4b2a658f88686d6bef8b78340eb1eb6009c41b466ab442505b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, embedding-reader, dataclasses, autofaiss\n",
            "Successfully installed autofaiss-2.15.3 dataclasses-0.6 embedding-reader-1.5.0 fire-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install faiss-cpu --no-cache\n",
        "!pip install autofaiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru7tpy_krFin"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odIpwutvq3TC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd3pxunWacqh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import faiss\n",
        "import glob\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "from operator import add\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqIkbaarw8mH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsgu-1OIe-dc"
      },
      "source": [
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5DxLMXeiCtC"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_covtype\n",
        "forest = fetch_covtype()\n",
        "Data= forest['data']\n",
        "label = forest['target']\n",
        "data_cov = np.concatenate([Data[:, :32], label.reshape(-1, 1)], axis = -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw96SFCmidmU",
        "outputId": "4d85472b-3c6e-4b0a-cb7f-7584a4d93d20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "data_cov = data_cov[:10000, :]\n",
        "data_cov.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWE8VXyHi7mk"
      },
      "outputs": [],
      "source": [
        "all_cov, query_cov = train_test_split(data_cov, test_size=0.2, random_state=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R0Wltz-idpG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOlRK4_Ve9zc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyTterQEwz0h"
      },
      "source": [
        "MR baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hB8zGB5acvg"
      },
      "outputs": [],
      "source": [
        "def Distance(test, train):\n",
        "    '''\n",
        "    Input:\n",
        "        - test: Numpy Array with label as last value\n",
        "        - train: Numpy Array with label as last value\n",
        "    \n",
        "    Returns distance between test and train arrays\n",
        "    '''\n",
        "    # dist = np.sqrt(np.sum((test[:-1]-train[:-1])**2))\n",
        "    # int(train[-1]), np.sum((test[:-1] - train[:-1]) ** 2)\n",
        "    return np.sum(np.subtract(test[:-1],train[:-1]) ** 2), int(train[-1])\n",
        "\n",
        "def MR_baseline(all_data_np, query_data_np, K):\n",
        "    print('current K = ', K)\n",
        "    spark = SparkSession\\\n",
        "            .builder\\\n",
        "            .appName(\"PythonKNN\")\\\n",
        "            .getOrCreate()\n",
        "    \n",
        "    # spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
        "    sc = spark.sparkContext\n",
        "\n",
        "    sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "    df = pd.DataFrame(all_data_np)\n",
        "    all_data = spark.createDataFrame(df).rdd\n",
        "    all_data = all_data.map(lambda x: np.array(x))\n",
        "\n",
        "    df = pd.DataFrame(query_data_np)\n",
        "    query_data = spark.createDataFrame(df).rdd\n",
        "    query_data = query_data.map(lambda x: np.array(x))\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    accuracy = 0\n",
        "    count = 0\n",
        "    for test_point in query_data.collect():\n",
        "\n",
        "        distances = all_data.map(\n",
        "            lambda train_point: Distance(test_point, train_point))\n",
        "\n",
        "        # # print('distances = ', distances)\n",
        "        # for d in distances.collect():\n",
        "        #     print('d =', d)\n",
        "\n",
        "        k_nearest_neighbours = sc.parallelize(distances.takeOrdered(K, key = lambda p: p[0])).map(lambda x: (x[1], 1))\n",
        "        \n",
        "        k_nearest_predictions = k_nearest_neighbours.reduceByKey(\n",
        "            lambda x1, x2: x1 + x2)\n",
        "        \n",
        "        predict_label = k_nearest_predictions.takeOrdered(1,\n",
        "            key = lambda x: -x[1])[0][0]\n",
        "\n",
        "        if predict_label == test_point[-1]:\n",
        "            accuracy += 1\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    end_time = time.time()\n",
        "    print('accuracy = ', accuracy / count)\n",
        "    time_taken = end_time - start_time\n",
        "    print('time_taken = ', time_taken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKO-kgHnYMQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5ecdcc-2edd-462d-8a67-50f494d61c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current K =  8\n",
            "accuracy =  0.767\n",
            "time_taken =  854.0006308555603\n",
            "current K =  16\n",
            "accuracy =  0.736\n",
            "time_taken =  833.285843372345\n",
            "current K =  32\n",
            "accuracy =  0.708\n",
            "time_taken =  844.1968381404877\n",
            "current K =  64\n",
            "accuracy =  0.6535\n",
            "time_taken =  829.3452432155609\n"
          ]
        }
      ],
      "source": [
        "for k in [8, 16,32,64]:\n",
        "  MR_baseline(all_cov, query_cov, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeNjDjNmd6jf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhEhW0sl3Z9E"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}